---
title: Use of new technologies
date: 2025-06-13
---

- Having a consistent approach is valuable; ideally same language, tools, libraries, patterns in code.
- Having older code “embedded” inside a newer tech stack is at best a temporary solution – worst case you end up with severe nesting, and changing anything inside it is tricky.
- Having >1 choices may mean a fragmented ecosystem, with none of the options working particularly well.

I appreciate consistency. This may be a character trait?

The [Choose boring technology essay](https://mcfunley.com/choose-boring-technology) is relevant here.

## Experience

In a prior role, all the firmware from the last decade was written in roughly the same style, using the same libraries. This meant that I could switch to some firmware that I’d never worked on and quickly find my way around – because the basic approach, language and core libraries were the same.

In the same role, one codebase (C#) had been ported to a new machine (C++ and Qt). The old codebase was still in use. This was annoying – changes had to be ported to both implementations. However, doing so wasn’t difficult because it was a direct port rather than a rewrite; the patterns of code and things you had to remember were the same.

The same role had good isolation between machine codebases, which meant that legacy concerns were limited. It also meant that porting fixes to older machines was largely on a “best effort” basis.

Microsoft has a habit of ending up with several API choices for anything. Consider the UI options or C# async styles. These tend to fall into three categories: old and somewhat broken, “deprecated” but functional and complete, and new and shiny but not complete. Generally the “last new thing” is the best choice, because the new thing may be abandoned on short notice and isn’t complete, and the really old thing is best avoided because it’s unsupported and awkward to work with (there is a reason it was replaced). We want to avoid this sort of thing in our own code.

Having deeply buried old software is awkward – it creates a split environment, and the buried stuff is hard to change. One system I looked at had the beating heart of the machine being an ancient, largely irrelevant application running in DosBox on a Linux box communicating over serial with the rest of the system. At some point developing for such a beast becomes sufficiently painful that it’s better to throw it away and start again.

Having a small number of technology choices simplifies things for hardware as well. There are benefits like being able to share parts between machines, simplified purchasing and storage of parts, and hopefully cost benefits from working at scale. There’s also better scope for process improvements – finding a cheaper supplier will improve the cost of all machines, not just one. However, generic hardware can cost more. It may be more expensive than required for the application and may miss out on some technical innovations. Like software, this must be balanced.

## Introducing new technologies

One option is to aim to have max two technology choices at once – the “current main” software, and then the other new/legacy option. If we then say “to introduce a new technology, you first have to remove one” we’d have a stable state with two technologies – a manageable number – and a way to introduce new technologies if a better one appears (get rid of an older one first).

The exception here is that if the problem domains are sufficiently different it’s probably fine to have a different technology – forcing use of a tool with minimal consistency benefits (because the usage is totally different) is probably not a good idea. For instance, it’s probably fine to have a scripting language that is different from the application development language – they are used for quite different purposes, so there isn’t much advantage of having things be the same.

It may turn out that “max two” is too small. If a product has a very long life cycle, it may be desirable to allow innovation while needing to support >1 older existing solutions. Provided that the split is clean, this is probably OK – the aim is to have a manageable number with a clear phase out plan.

A new technology should be a revolution, not an improvement. Consistency is more important than partial perfection, but if you can’t do something with the old technology, you have to do something different. For instance, I’m unsure that Rust is so much of an improvement that it is worth having two main languages. On the other hand, I think that machine learning is so much better than traditional CPU processing that it’s worth pursuing.

